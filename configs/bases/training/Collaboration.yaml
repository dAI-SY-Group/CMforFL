#TRAINING
training:
  environment: SiloFederatedAveragingEnvironment
  trainer: PFLTrainer
  client_class: PFLClient
  personal_model_head:
    input_dim: 256
  num_clients: 5
  weigh_sample_quantity: True
  epochs: 1
  save_rounds: [] #save a checkpoint every ... rounds (epochs); set equal to epochs if you dont want sepcific epoch states (LOCAL)
  batch_size: 256
  loss: CrossEntropy
  metrics: [SKAccuracy, BalancedAccuracy] #List of Train Metrics to look at during training
  validation_frequency: 1 #number of rounds after which the model will be evaluated on the data given as tst_data (val data)
  optimizer: 
    name: Adam
    lr: 0.0005
    beta1: 0.9 
    beta2: 0.999 
    weight_decay: 0
  lr_scheduler:  False
  early_stopping: #Local Early Stopper only used for tracking best model state
    patience: 100
    delta: 0 # Minimum change in the monitored quantity to qualify as an improvement
    metric: CrossEntropy
    use_loss: True
    subject_to: min #min / max
    verbose: True
  random_client_start: False
  tuning: 
    epochs: 5
    lr: 0.00005
  glob:
    communication_rounds: 1000
    validation_frequency: 1
    save_rounds: [] #[1, 10, 25, 50, 100] #save a checkpoint every ... rounds (epochs); set equal to epochs if you dont want sepcific epoch states (GLOBAL)
    early_stopping: #False or dict with parameters
      patience: 100
      delta: 0 # Minimum change in the monitored quantity to qualify as an improvement
      metric: CrossEntropy
      use_loss: True
      subject_to: min #min / max
      verbose: True
data_distribution_config:
  partition_mode: IID

#EVAL
eval:
  skip_test_data_during_training: True
  local_models: True
  available_classes_test_data: True
  local_splits: True